{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDGAR-P",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNVFv7qzdPzpNvbKC6yadq1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LouisCastricato/EDGAR-P/blob/main/EDGAR_P.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wenjIALmwPCm"
      },
      "source": [
        "# Installs/Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dTkUGg14vpL",
        "outputId": "d7ec428e-d070-4c85-9dba-ef8abda69f74"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ucQ8UeA-uwg",
        "outputId": "e7d01f92-23a2-43b0-faa8-250afe95ccd1"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogqi6NjBiTFh",
        "outputId": "bd8e5325-7e89-44d0-b5b3-4f313430d223"
      },
      "source": [
        "!pip install transformers sentencepiece textacy auto-tqdm graphviz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n",
            "Requirement already satisfied: textacy in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: auto-tqdm in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.5.1)\n",
            "Requirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.11.0)\n",
            "Requirement already satisfied: jellyfish>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.8.2)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.22.2.post1)\n",
            "Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.10.0)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (3.0.6)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.4.1)\n",
            "Requirement already satisfied: environments-utils in /usr/local/lib/python3.7/dist-packages (from auto-tqdm) (1.0.3)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (from auto-tqdm) (0.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->textacy) (4.4.2)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.10.1->textacy) (0.11.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.7.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (8.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.3.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.5.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (56.0.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.4.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->textacy) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_dm3yDawBry"
      },
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "import textacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s7d6ZQXemmC"
      },
      "source": [
        "#We're using Neo for this tutorial\n",
        "from transformers import GPTNeoModel, GPTNeoForCausalLM,\\\n",
        "    GPT2Tokenizer, GPTNeoConfig\n",
        "import torch\n",
        "from transformers import (\n",
        "  StoppingCriteriaList,\n",
        "  MinLengthLogitsProcessor,\n",
        "  MaxLengthCriteria,\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  LogitsProcessorList,\n",
        "  MaxTimeCriteria,\n",
        "  ForcedEOSTokenLogitsProcessor,\n",
        ")\n",
        "import transformers\n",
        "transformers.logging.set_verbosity(transformers.logging.CRITICAL)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6JTEzdrgnHX"
      },
      "source": [
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "model.config.pad_token_id = model.config.eos_token_id\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNhSx2q4kaIb"
      },
      "source": [
        "%%capture\n",
        "model = model.to(\"cuda\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rRSf5BDMSes"
      },
      "source": [
        "from transformers.generation_logits_process import LogitsProcessor,\\\n",
        "NoBadWordsLogitsProcessor, NoRepeatNGramLogitsProcessor, RepetitionPenaltyLogitsProcessor\n",
        "\n",
        "class HorizonRepetitionPenalty(LogitsProcessor):\n",
        "  def __init__(self, penalty: float, horizon: torch.LongTensor, horizon_exclusive = False):\n",
        "    if not isinstance(penalty, float) or not (penalty > 0):\n",
        "      raise ValueError(f\"`penalty` has to be a strictly positive float, but is {penalty}\")\n",
        "\n",
        "    self.penalty = penalty\n",
        "    self.horizon=horizon\n",
        "    self.exclusive=horizon_exclusive\n",
        "  def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "    num_beams = input_ids.shape[0]\n",
        "    horizon = torch.cat(num_beams*[self.horizon], dim=0)\n",
        "    if not self.exclusive:\n",
        "      input_ids = torch.cat((input_ids, horizon), dim=-1)\n",
        "    else:\n",
        "      input_ids = horizon\n",
        "    for i in range(scores.shape[0]):\n",
        "      for previous_token in set(input_ids[i].tolist()):\n",
        "        # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability\n",
        "        if scores[i, previous_token] < 0:\n",
        "          scores[i, previous_token] *= self.penalty\n",
        "        else:\n",
        "          scores[i, previous_token] /= self.penalty\n",
        "    return scores"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAH-pCW3fg_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5862414d-94a5-46e0-d986-73c905789f3f"
      },
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    LogitsProcessorList,\n",
        "    MinLengthLogitsProcessor,\n",
        "    TopKLogitsWarper,\n",
        "    TemperatureLogitsWarper,\n",
        "    BeamSearchScorer,\n",
        ")\n",
        "\n",
        "bad_words_list = [\"Because\", \"Because,\", \"Because \", \"because\",\\\n",
        "                  \" Because\", \" Because,\", \" because\"\\\n",
        "                  \"Yes\", \"Yes,\", \"Yes \", \"yes\",\\\n",
        "                  \" Yes\", \" Yes,\", \" yes\",\\\n",
        "                  \" No\", \" No,\", \" no\",\\\n",
        "                  \"(\", \" (\", \")\", \") \"]\n",
        "bad_words_ids = list(map(lambda x: tokenizer(x)['input_ids'], bad_words_list))\n",
        "\n",
        "expl = [[1427],[2602, 834],[29343],[37405],[35780],[2602]]\n",
        "bad_words_ids += expl\n",
        "print(bad_words_ids)\n",
        "\n",
        "#Takes a model and computes the perplexity of the target sequence given the input sequence\n",
        "def perplexity(encodings, stride=1, m=model):\n",
        "  lls = []\n",
        "  inp_ids = encodings['input_ids']\n",
        "  start = encodings['start']\n",
        "  max_length = len(encodings['input_ids'].squeeze())\n",
        "  for i in range(start, inp_ids.size(1), stride):\n",
        "      begin_loc = max(i + stride - max_length, 0)\n",
        "      end_loc = min(i + stride, inp_ids.size(1))\n",
        "      trg_len = end_loc - i    # may be different from stride on last loop\n",
        "      input_ids = inp_ids[:,begin_loc:end_loc].to(\"cuda\")\n",
        "      target_ids = input_ids.clone()\n",
        "      target_ids[:,:-trg_len] = -100\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = m(input_ids, labels=target_ids)\n",
        "          log_likelihood = outputs[0] * trg_len\n",
        "      lls.append(log_likelihood)\n",
        "\n",
        "  return (torch.exp(torch.stack(lls).sum() / end_loc)).item()\n",
        "\n",
        "#Constructs a sequence for determining the perplexity of a target given a prompt\n",
        "def construct(prompt, target, force_start=None):\n",
        "  prompt_tok = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  target_tok = tokenizer(target, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  if force_start is None:\n",
        "    start = len(prompt_tok['input_ids'].squeeze())\n",
        "  else:\n",
        "    start = force_start\n",
        "  #Start encodes where the prompt sequence ends and target begins\n",
        "  return {\n",
        "      'input_ids': torch.cat((prompt_tok['input_ids'],target_tok['input_ids']), dim=-1).cuda(),\n",
        "      'attention_mask': torch.cat((prompt_tok['attention_mask'],target_tok['attention_mask']), dim=-1).cuda(),\n",
        "      'start':start,\n",
        "  }\n",
        "\n",
        "def generate(ids, max_length=1024, horizon=None, horizon_penalty=None, beams=2, extra_bad_words = None, repetition_penalty=2.0):\n",
        "  bad_words_t = bad_words_ids\n",
        "  if extra_bad_words is not None:\n",
        "    bad_words_t += extra_bad_words\n",
        "  model_out=None\n",
        "  if horizon is None:\n",
        "    model_out = model.generate(input_ids = ids['input_ids'],\\\n",
        "                               max_length=max_length, num_beams=beams,\\\n",
        "                               no_repeat_ngram_size=5, bad_words_ids=bad_words_t, repetition_penalty=repetition_penalty)[0]\n",
        "  else:\n",
        "    horizon_ids = tokenizer(horizon, return_tensors=\"pt\")['input_ids'].cuda()\n",
        "    input_ids = ids[\"input_ids\"]\n",
        "    model.config.max_length = max_length\n",
        "    # instantiate logits processors\n",
        "    logits_processor = LogitsProcessorList([\n",
        "        MinLengthLogitsProcessor(ids['input_ids'].shape[1], model.config.eos_token_id),\n",
        "        NoRepeatNGramLogitsProcessor(5),\n",
        "        NoBadWordsLogitsProcessor(bad_words_t, eos_token_id=model.config.eos_token_id),\n",
        "        HorizonRepetitionPenalty(penalty=horizon_penalty, horizon=horizon_ids, horizon_exclusive=True),\n",
        "        RepetitionPenaltyLogitsProcessor(penalty=repetition_penalty)\n",
        "    ])\n",
        "    stopping_criteria = StoppingCriteriaList([\n",
        "        MaxLengthCriteria(max_length=max_length),\n",
        "    ])\n",
        "    model_kwargs={\n",
        "        \"attention_mask\":ids['attention_mask'],\n",
        "        \"use_cache\":True,\n",
        "    }\n",
        "    with torch.no_grad():\n",
        "      model_out = model.greedy_search(\n",
        "          input_ids=ids[\"input_ids\"], logits_processor=logits_processor,\\\n",
        "          stopping_criteria=stopping_criteria)[0]\n",
        "    \n",
        "  return tokenizer.decode(model_out)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8128], [8128, 11], [8128, 220], [13893], [4362], [4362, 11], [780, 5297], [5297, 11], [5297, 220], [8505], [3363], [3363, 11], [3763], [1400], [1400, 11], [645], [7], [357], [8], [8, 220], [1427], [2602, 834], [29343], [37405], [35780], [2602]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSXYKW0Pk0X1"
      },
      "source": [
        "# Ranker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjP65xqfpUFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6949db90-a41a-4685-ca6a-9f85e07c3d1b"
      },
      "source": [
        "%mkdir distilgpt2-ranker-roc\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/distilgpt2-roc/config.json distilgpt2-ranker-roc/config.json\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/distilgpt2-roc/training_args.bin distilgpt2-ranker-roc/training_args.bin\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/distilgpt2-roc/pytorch_model.bin distilgpt2-ranker-roc/pytorch_model.bin\n",
        "\n",
        "%mkdir distilgpt2-ranker-scifi\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/distilgpt2-scifi/config.json distilgpt2-ranker-scifi/config.json\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/distilgpt2-scifi/training_args.bin distilgpt2-ranker-scifi/training_args.bin\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/distilgpt2-scifi/pytorch_model.bin distilgpt2-ranker-scifi/pytorch_model.bin\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘distilgpt2-ranker-roc’: File exists\n",
            "mkdir: cannot create directory ‘distilgpt2-ranker-scifi’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxAHAAHvpU0K",
        "outputId": "c6e477e8-9b4e-4a62-8910-846d7119c645"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "special_tokens_dict = {'prompt' : '<pmpt>'}\n",
        "#model_name = \"distilgpt2-ranker-roc/\"\n",
        "#Download models\n",
        "#tokenizer_roc =  AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "#model_roc = AutoModelWithLMHead.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "model_name = \"distilgpt2-ranker-scifi/\"\n",
        "#Download models\n",
        "tokenizer_scifi =  AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "model_scifi = AutoModelWithLMHead.from_pretrained(model_name).to(\"cuda\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:762: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE68sx_j72MA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0461bcbf-55de-46c9-f99d-3c7e63c087b3"
      },
      "source": [
        "import transformers\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "\n",
        "rep = transformers.RepetitionPenaltyLogitsProcessor(1.1)\n",
        "\n",
        "def compute_loss(logits, labels):\n",
        "  # Shift so that tokens < n predict n\n",
        "  shift_logits = logits[..., :-1, :].contiguous()\n",
        "  shift_labels = labels[..., 1:].contiguous()\n",
        "  # Flatten the tokens\n",
        "  loss_fct = CrossEntropyLoss()\n",
        "  return loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "\n",
        "\n",
        "#Computes perplexity using a rep penalty\n",
        "def perplexity_w_rep(encodings, stride=1, m=None):\n",
        "  lls = []\n",
        "  inp_ids = encodings['input_ids']\n",
        "  start = encodings['start']\n",
        "  max_length = len(encodings['input_ids'].squeeze())\n",
        "  for i in range(start, inp_ids.size(1), stride):\n",
        "      begin_loc = max(i + stride - max_length, 0)\n",
        "      end_loc = min(i + stride, inp_ids.size(1))\n",
        "      trg_len = end_loc - i    # may be different from stride on last loop\n",
        "      input_ids = inp_ids[:,begin_loc:end_loc].to(\"cuda\")\n",
        "      target_ids = input_ids.clone()\n",
        "      target_ids[:,:-trg_len] = -100\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = m(input_ids, labels=target_ids)\n",
        "          logits = outputs.logits.squeeze()\n",
        "          for i in range(1, len(logits)):\n",
        "            #print(i)\n",
        "            #print(logits[i].shape)\n",
        "            ids_t = input_ids[0, :i].unsqueeze(0)\n",
        "            logits_t = logits[i].unsqueeze(0)\n",
        "            logits[i] = rep(ids_t, logits_t).squeeze()\n",
        "          loss = compute_loss(logits.unsqueeze(0), target_ids)\n",
        "          #logits = rep(input_ids, logits)\n",
        "\n",
        "          #print(target_ids.shape)\n",
        "          #print(input_ids.shape)\n",
        "\n",
        "          log_likelihood = loss * trg_len\n",
        "      lls.append(log_likelihood)\n",
        "\n",
        "  return (torch.exp(torch.stack(lls).sum() / end_loc)).item()\n",
        "\n",
        "\n",
        "def rank(string, force_start=2, model=model_scifi):\n",
        "  #Pull out the last word to use the construct function\n",
        "  t1 = string.split()\n",
        "  t2 = \" \".join(t1[-1:])\n",
        "  t1 = \" \".join(t1[:-1])\n",
        "\n",
        "  #Filtering out the first few tokens helps significantly. so force_start = 3\n",
        "  return perplexity_w_rep(construct(t1, t2, force_start = force_start), m=model) \n",
        "\n",
        "rank(\"Karen was assigned a roommate her first year of college. Her roommate asked her to go to a nearby city for a concert.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68.33157348632812"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD8mUQWS4qg9"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PAMzctCXrKA",
        "outputId": "7bfba099-1709-4aa5-cb01-29c4e49ac9d9"
      },
      "source": [
        "questions_bad_words = [\"What\", \" What\", \"\\nWhat\", \" what\", \"what\"]\n",
        "questions_bad_words_ids = list(map(lambda x: tokenizer(x)['input_ids'], questions_bad_words))\n",
        "\n",
        "def clean_story(story):\n",
        "  #Need to remove colons and line breaks\n",
        "  story = story.replace(\":\", \"-\")\n",
        "  story = story.replace(\"\\n\\n\", \"\")\n",
        "  story = story.replace(\"\\n\", \" \")\n",
        "  story = story.replace(\"<|endoftext|>\", \"\")\n",
        "  return story\n",
        "\n",
        "def get_questions(story):\n",
        "  instructions = \"You will be given a set of short stories and asked to write a set of questions.\\n\\\n",
        "  Please write questions, in no particular order, that when answered tell the events leading up to the story.\\n\"\n",
        "  story1 = \"Story 1: John went for a swim.\\n\"\n",
        "  good1 = \"Good Questions: How did John get to the swimming pool? What happened before John went swimming? Why did John go swimming?\\n\"\n",
        "  bad1 = \"Bad Questions: Who is John? What did the pool water taste like? What happened after John went swimming? What does John do now?\\n\\n\"\n",
        "\n",
        "\n",
        "  story2 = \"Story 2: The walk to school that day was long but, Tom was motivated to give Jim back his book. Tom gave the book to Jim.\\n\"\n",
        "  good2 = \"Good Questions: Why was Tom motivated? How did Tom get the book? Why did Tom give Jim the book? Why did Jim want the book?\\n\"\n",
        "  bad2 = \"Bad Questions: Who were they? Did Jim want the book? How are they similar? What is Tom wearing? When did Tom give Jim the book? Where is the book? What happens next? What is Jim going to do once he gets the book?\\n\\n\"\n",
        "\n",
        "  story3 = \"Story 3: Mary was so happy to have finally crossed the street.\\n\"\n",
        "  good3 = \"Good Questions: Why did Mary cross the street? Why was Mary unhappy? Why was Mary running from someone?\\n\"\n",
        "  bad3 = \"Bad Questions: What is Mary like? What does Mary do after crossing the street? What does Mary do now that she is happy? What happens next?\\n\\n\"\n",
        "\n",
        "  story4 = \"Story 4: \" + story + \"\\n\"\n",
        "  \n",
        "  inp = instructions + story1 + good1 +  bad1 + story2 + good2 + bad2 + story3 + good3 + bad3 + story4\n",
        "\n",
        "\n",
        "  out = generate(construct(inp, \"Good Questions: Why\"), max_length=512, repetition_penalty=2.8, extra_bad_words=questions_bad_words_ids)\n",
        "  #Get questions out\n",
        "  out = out.split(\":\")[11].split(\"?\")[:-1]\n",
        "  out = \"?\".join(out)\n",
        "  if out[-1] != \"?\":\n",
        "    out+=\"?\"\n",
        "  out = sent_tokenize(out)\n",
        "  good_questions = list()\n",
        "  #As soon as we find a bad question, break\n",
        "  for string in out:\n",
        "    if not (\"Bad Question\" in string):\n",
        "      good_questions.append(string)\n",
        "    else:\n",
        "      break\n",
        "  #Remove the space from the first question\n",
        "  if good_questions[0][0] == ' ':\n",
        "    good_questions[0] = good_questions[0][1:]\n",
        "  return good_questions\n",
        "\n",
        "input_story = \"They wanted to see what happened outside.\"\n",
        "\n",
        "get_questions(clean_story(input_story))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Why would they want to see if there was something outside?',\n",
              " 'Why would they not want to see anything outside?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtBbu4DiznUd",
        "outputId": "748639cf-bbe7-4244-eaef-c549037bf632"
      },
      "source": [
        "def continue_story(story, question, width = 10):\n",
        "  instructions = \"You will be given a set of short stories and question.\\nThe question asks about what happens before the story starts.\\nList plausible answers for the question\\n\"\n",
        "  story1=\"Story 1: Jim sat by the swings as Tom slowly approached. Tom gave the book to Jim during recess.\\n\"\n",
        "  question1=\"Question: How did Tom get the book?\\n\"\n",
        "  answer1=\\\n",
        "\"Correct Answers:\\n\\\n",
        "1. Jim gave Tom the book because Tom is his best friend.\\n\\\n",
        "2. Tom is a thief, so he stole the book from Jim.\\n\\\n",
        "3. Tom took the book from the bully that beat up Jim.\\n\\\n",
        "4. Tom noticed that the book fell off Jim's lap as he played on the swings.\\n\\\n",
        "Wrong Answers:\\n\\\n",
        "1. Jim ran from Tom.\\n\\\n",
        "2. Jim wanted something to drink.\\n\\\n",
        "3. Jim sat patiently in his bedroom.\\n\\\n",
        "4. Jim was not going to give Tom the book.\\n\\\n",
        "5. Because Jim gave it to him.\\n\\n\"\n",
        "  story2 = \"Story 2: Mary was so happy to have finally crossed the street.\\n\"\n",
        "  question2 = \"Question: What was Mary running from?\\n\"\n",
        "  answer2=\\\n",
        "\"Correct Answers:\\n\\\n",
        "1. Mary's heart pounded. As she looked over her shoulder, she saw it.\\n\\\n",
        "2. Mary had been trying to avoid them all day. If they knew she was skipping school she would be in trouble.\\n\\\n",
        "3. The stop sign was the finish line, she only had a few hundred more feet to go.\\n\\\n",
        "Wrong Answers:\\n\\\n",
        "1. Mary was in the desert where there are no roads.\\n\\\n",
        "2. Mary was going for a joyful stroll through the park.\\n\\\n",
        "3. Mary was indifferent about her pursuer.\\n\\n\" \n",
        "  story3 = \"Story 3: \" + story + \"\\n\"\n",
        "  question3 = \"Question: \" + question +\"\\n\"\n",
        "  inp = instructions + story1 + question1 +  answer1 + story2 + question2 + answer2 + story3 + question3\n",
        "\n",
        "\n",
        "  out = generate(construct(inp, \"Correct Answers:\\n1.\"), max_length=1024, horizon=story, horizon_penalty=3.0)\n",
        "  out = out.split(\"Story 3\")[1]\n",
        "  #Remove the next story\n",
        "  out = out.split(\"Story 4\")[0]\n",
        "  #Filter to correct answers\n",
        "  out = out.split(\"Correct Answers:\")[1]\n",
        "  out = out.split(\"Wrong\")[0]\n",
        "\n",
        "  #print(out)\n",
        "  #If the user does not specify a width\n",
        "  if width == -1:\n",
        "    #Capture all of them\n",
        "    width = 100\n",
        "  responses = list()\n",
        "  #Reads through the outputted list and returns every item\n",
        "  for i in range(1, width):\n",
        "    try:\n",
        "      start = \"\\n\"+str(i)\n",
        "      end = \"\\n\"+str(i+1)\n",
        "      responses.append(out.split(start)[1].split(end)[0])\n",
        "    except:\n",
        "      break\n",
        "  #Remove first space\n",
        "  for i in range(len(responses)):\n",
        "    if responses[i][:2] == '. ':\n",
        "      responses[i] = responses[i][2:] \n",
        "    elif responses[i][:2] == ') ':\n",
        "      responses[i] = responses[i][2:]\n",
        "    elif responses[i][0] == ' ':\n",
        "      responses[i] = responses[i][1:]\n",
        "\n",
        "    responses[i] = \" \".join(responses[i].split())\n",
        "    responses[i] = clean_story(responses[i])\n",
        "  return responses\n",
        "inp_story = \"They felt lucky they had evacuated when they did.\"\n",
        "starts = continue_story(inp_story, \"What did Karen do to get to the concert?\")\n",
        "print(starts)\n",
        "new_stories = list(map(lambda x: x+\" \"+inp_story, starts))\n",
        "print(new_stories)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['She walked home from school.', 'She rode her bike to school.', 'She walked home with her friends.']\n",
            "['She walked home from school. They felt lucky they had evacuated when they did.', 'She rode her bike to school. They felt lucky they had evacuated when they did.', 'She walked home with her friends. They felt lucky they had evacuated when they did.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKE6vnQVevT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef93b0a1-eefd-40e3-d7c7-0404a47404fe"
      },
      "source": [
        "continue_bad_words_list = [\"Wrong\", \"\\nWrong\", \" Wrong\",\\\n",
        "                           \"Wrongs\", \"\\nWrongs\", \" Wrongs\",\\\n",
        "                           \"WrONG\", \"\\nWrONG\", \" WrONG\",\\\n",
        "                           \"Wrond\", \"\\nWrond\", \" Wrond\",\\\n",
        "                           \"wrong\", \"\\nwrong\", \" wrong\"]\n",
        "continue_bad_words_list_ids = list(map(lambda x: tokenizer(x)['input_ids'], continue_bad_words_list))\n",
        "\n",
        "#Generates SVO tuples continuing the story\n",
        "def continue_story_svo(story, question, width = 10):\n",
        "  instructions = \"You will be given a set of short stories and question.\\nThe question asks about what happens before the story starts.\\nList plausible answers for the question without contradicting the story.\\n\"\n",
        "  story1=\"Story 1: Jim sat by the swings as Tom slowly approached. Tom gave the book to Jim during recess.\\n\"\n",
        "  question1=\"Question: Why did Jim recieve the book?\\n\"\n",
        "  answer1=\\\n",
        "\"Correct Answers:\\n\\\n",
        "1. Tom desired to return the book to his friend.\\n\\\n",
        "2. Jim bought the book from Tom.\\n\\\n",
        "3. Tom noticed Jim dropped his book.\\n\\\n",
        "4. Jim needed a book to study for his exam.\\n\\\n",
        "5. Tom wanted to give Jim his favorite book.\\n\\\n",
        "Wrong Answers:\\n\\\n",
        "1. Jim did not want the book.\\n\\\n",
        "2. Tom noticed Jim was sitting alone during lunch.\\n\\\n",
        "3. Jim sat patiently in his bedroom.\\n\\\n",
        "4. Jim was not going to give Tom the book.\\n\\n\"\n",
        "  story2 = \"Story 2: Mary was happy to finally cross the street.\\n\"\n",
        "  question2 = \"Question: Why was Mary running?\\n\"\n",
        "  answer2=\\\n",
        "\"Correct Answers:\\n\\\n",
        "1. Mary was running from a monster.\\n\\\n",
        "2. Mary was running a marathon.\\n\\\n",
        "3. Mary wanted to get away from her parents.\\n\\\n",
        "Wrong Answers:\\n\\\n",
        "1. Mary was in the desert where there are no roads.\\n\\\n",
        "2. Mary was going for a joyful stroll through the park.\\n\\\n",
        "3. Mary was indifferent about her pursuer.\\n\\\n",
        "4. Mary was running on a road.\\n\\n\" \n",
        "  story3 = \"Story 3: \" + story + \"\\n\"\n",
        "  question3 = \"Question: \" + question +\"\\n\"\n",
        "  inp = instructions + story1 + question1 +  answer1 + story2 + question2 + answer2 + story3 + question3\n",
        "  #print(inp)\n",
        "\n",
        "  out = generate(construct(inp, \"Correct Answers:\\n1.\"), max_length=512, horizon=story, horizon_penalty=3.5, repetition_penalty=2.0)\n",
        "  #print(out)\n",
        "  out = out.split(\"Story 3\")[1]\n",
        "  #Remove the next story\n",
        "  out = out.split(\"Story 4\")[0]\n",
        "  #Filter to correct answers\n",
        "  out = out.split(\"Correct Answers:\")[1]\n",
        "  out = out.split(\"Wrong\")[0]\n",
        "\n",
        "  #If the user does not specify a width\n",
        "  if width == -1:\n",
        "    #Capture all of them\n",
        "    width = 100\n",
        "  responses = list()\n",
        "  #Reads through the outputted list and returns every item\n",
        "  for i in range(1, width):\n",
        "    try:\n",
        "      start = \"\\n\"+str(i)\n",
        "      end = \"\\n\"+str(i+1)\n",
        "      responses.append(out.split(start)[1].split(end)[0])\n",
        "    except:\n",
        "      break\n",
        "  #Remove first space\n",
        "  for i in range(len(responses)):\n",
        "    if responses[i][:2] == '. ':\n",
        "      responses[i] = responses[i][2:] \n",
        "    elif responses[i][:2] == ') ':\n",
        "      responses[i] = responses[i][2:]\n",
        "    elif responses[i][0] == ' ':\n",
        "      responses[i] = responses[i][1:]\n",
        "\n",
        "    responses[i] = \" \".join(responses[i].split())\n",
        "    responses[i] = clean_story(responses[i])\n",
        "  return responses\n",
        "inp_story = \"They felt lucky they had evacuated when they did.\"\n",
        "starts = continue_story_svo(inp_story, \"Why would they want to see what happened out there?\")\n",
        "print(starts)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['They wanted to see if the evacuation was successful.', 'They wanted to know if the evacuation was safe.', 'They wanted to find out if the evacuation was complete.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wUQZaHkFDJ_"
      },
      "source": [
        "def rank_sort(stories, model=model_scifi):\n",
        "  ranks = list(map(lambda x: rank(x, force_start=1, model=model), stories))\n",
        "  ranked_stories = zip(stories, ranks)\n",
        "  sorted_stories = sorted(ranked_stories, key=lambda x: x[1])\n",
        "  return list(map(lambda x: x[0], sorted_stories))\n",
        "#print(rank_sort(new_stories)[0])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTggpkrSwvir"
      },
      "source": [
        "# EDGAR-P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74SeBiXVw6E9"
      },
      "source": [
        "#Determines if we should reject based off of if it has an SVO\n",
        "def hasSVO(sent):\n",
        "  text = nlp(sent)\n",
        "  ex =  textacy.extract.subject_verb_object_triples(text)\n",
        "  for i in ex:\n",
        "    return True\n",
        "  return False\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LOgJVNB-DDr"
      },
      "source": [
        "#Determines if one statement implies another\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "#This stays on CPU\n",
        "tokenizer_deberta = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge-mnli\")\n",
        "model_deberta = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v2-xxlarge-mnli\")\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sdkmNJ6-Pqd"
      },
      "source": [
        "import numpy as np\n",
        "def not_contradict(sentA, sentB):\n",
        "  to_run = \"[CLS]\" + sentA  + \"[SEP]\" + sentB + \"[SEP]\"\n",
        "  inputs = tokenizer_deberta(to_run, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "    outputs = model_deberta(**inputs)\n",
        "\n",
        "  outputs = np.array(outputs.logits.squeeze().cpu().tolist())\n",
        "  choice = np.argmax(outputs)\n",
        "  if choice == 0:\n",
        "    return -100\n",
        "  return outputs[2]\n",
        "  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51-aLgKDgBx1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "6939caeb-f40c-46fb-c891-1ba837ba1338"
      },
      "source": [
        "print(not_contradict(\"The sky is blue.\", \"The sky is not blue.\"))\n",
        "print(not_contradict(\"The sky is blue.\", \"The sky is not red.\"))\n",
        "print(not_contradict(\"The sky is blue.\", \"The ground is green.\"))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-41887485ad00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_contradict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The sky is blue.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The sky is not blue.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_contradict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The sky is blue.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The sky is not red.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_contradict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The sky is blue.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The ground is green.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-be733600bf24>\u001b[0m in \u001b[0;36mnot_contradict\u001b[0;34m(sentA, sentB)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_deberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_deberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         )\n\u001b[1;32m   1067\u001b[0m         \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0mquery_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0mrelative_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             )\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, return_att, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_att\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_att\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW7L0jL1F60T"
      },
      "source": [
        "questions = get_questions(inp_story)\n",
        "extensions = list()\n",
        "for q in questions:\n",
        "  extensions += continue_story_svo(inp_story, q)\n",
        "#Do we have SVO\n",
        "extensions = list(filter(hasSVO, extensions))\n",
        "new_stories = list(map(lambda x: x+\" \"+inp_story, extensions))\n",
        "sorted_l = rank_sort(new_stories)\n",
        "print(sorted_l[0])\n",
        "inp_story=sorted_l[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTI7h2jiQvyq"
      },
      "source": [
        "from auto_tqdm import tqdm\n",
        "\n",
        "#Beams should be [inp_story] when we start\n",
        "def beam_search(beams, width=20, diversity_width=2, graph=None):\n",
        "  candidates = list()\n",
        "  for story in tqdm(beams):\n",
        "    story_sents = sent_tokenize(story)\n",
        "    #print(story)\n",
        "\n",
        "    questions = get_questions(story)\n",
        "\n",
        "    extensions = list()\n",
        "    for q in questions:\n",
        "      continuation = continue_story_svo(story, q, width=-1)\n",
        "      q_cur = [q]*len(continuation)\n",
        "      extensions += zip(continuation, q_cur)\n",
        "    #Sort by most likely to imply and take the top k\n",
        "    implication_story = \" \".join(story_sents[0:min(len(story_sents), 3)]) #The first three sentences sliding window\n",
        "    #extensions = list(filter(hasSVO, extensions)) #Filter on if there is an SVO tuple\n",
        "    extensions_ranks = list(map(lambda x: not_contradict(x[0], implication_story), extensions)) #Rank\n",
        "    extensions_zip = list(filter(lambda x: x[1] > -100, zip(extensions, extensions_ranks))) #Zip\n",
        "    extensions_zip = sorted(extensions_zip, key=lambda x: x[1], reverse=True)\n",
        "    extensions = list(map(lambda x: x[0], extensions_zip)) #Sort\n",
        "\n",
        "    extensions = extensions[:min(diversity_width, len(extensions))] #Take top k\n",
        "    new_stories = list(map(lambda x: x[0]+\" \"+story, extensions)) #Concat\n",
        "\n",
        "    #Debug mode\n",
        "    if graph is not None:\n",
        "      for i, s in enumerate(new_stories):\n",
        "        graph.node(extensions[i][0])\n",
        "        graph.edge(story_sents[0], extensions[i][0], label=extensions[i][1])\n",
        "\n",
        "    #print(\"\\n\".join(new_stories))\n",
        "\n",
        "    #Internally rank the new stories to preserve diversity\n",
        "    #new_stories = rank_sort(new_stories)[0:min(diversity_width, len(new_stories))]\n",
        "    candidates += new_stories\n",
        "\n",
        "  sorted_l = rank_sort(candidates, model=model_scifi)\n",
        "  #print(sorted_l[0])\n",
        "  return sorted_l[:min(len(sorted_l), width)], graph"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "toQdrN80SwW2",
        "outputId": "e8680485-28c5-4593-aa21-d8e0349cc430"
      },
      "source": [
        "from graphviz import Digraph\n",
        "from IPython.core.display import display, HTML\n",
        "f = Digraph('transition_graph', filename='tg.png')\n",
        "f.format='png'\n",
        "f.engine = 'dot'\n",
        "f.ratio=\"fill\"\n",
        "f.node_attr['fixedsize'] = 'false'\n",
        "f.attr(rankdir='TB', size='100,100')\n",
        "\n",
        "inp_story=\"They felt lucky they had evacuated when they did..\"\n",
        "#Initial vertex\n",
        "f.node(sent_tokenize(inp_story)[0])\n",
        "\n",
        "\n",
        "beams, f = beam_search([inp_story], width=5, diversity_width=2, graph=f)\n",
        "for i in range(5):\n",
        "  f.render()\n",
        "  #display(HTML(f.svg()))\n",
        "\n",
        "  #print(beams)\n",
        "  beams = list(set(beams))\n",
        "  print(\"\\nSTEP: \" + str(i) + \"\\n\\n\")\n",
        "  print(\"\\n\".join(beams))\n",
        "  beams, f = beam_search(beams, width=5, diversity_width=2,graph=f)\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [05:41<00:00, 341.25s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "STEP: 0\n",
            "\n",
            "\n",
            "They were lucky because they were able to evacuate. They felt lucky they had evacuated when they did..\n",
            "They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 50%|█████     | 1/2 [06:57<06:57, 417.88s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 2/2 [09:44<00:00, 392.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "STEP: 1\n",
            "\n",
            "\n",
            "The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n",
            "The school was on fire! They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n",
            "The storm was moving so fast that they left early. They were lucky because they were able to evacuate. They felt lucky they had evacuated when they did..\n",
            "The storm was coming closer so they left early. They were lucky because they were able to evacuate. They felt lucky they had evacuated when they did..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 25%|██▌       | 1/4 [05:16<15:49, 316.35s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 2/4 [12:26<10:55, 327.69s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 3/4 [18:24<05:30, 330.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 4/4 [21:10<00:00, 314.29s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "STEP: 2\n",
            "\n",
            "\n",
            "The storm made it easy for them to escape. The storm was moving so fast that they left early. They were lucky because they were able to evacuate. They felt lucky they had evacuated when they did..\n",
            "There is never an evacuation drill. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n",
            "The storm was very dangerous for them to stay at home. The storm was coming closer so they left early. They were lucky because they were able to evacuate. They felt lucky they had evacuated when they did..\n",
            "The storm was approaching too quickly for them to leave later. The storm was coming closer so they left early. They were lucky because they were able to evacuate. They felt lucky they had evacuated when they did..\n",
            "The siren is too loud. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [07:44<30:56, 464.23s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [10:03<21:35, 431.71s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [13:34<13:39, 409.62s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [18:51<06:40, 400.36s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [22:03<00:00, 379.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "STEP: 3\n",
            "\n",
            "\n",
            "They were lucky because they escaped the fire at the last minute. There is never an evacuation drill. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n",
            "The firemen have left the building because the siren hasn't been turned on. The siren is too loud. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n",
            "They were lucky their house wasn’t destroyed by the storm. The storm was approaching too quickly for them to leave later. The storm was coming closer so they left early. They were lucky because they were able to evacuate. They felt lucky they had evacuated when they did..\n",
            "The firemen thought the siren sounded like a car alarm, but it wasn't. The siren is too loud. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n",
            "They were lucky, because they didn’t have to evacuate until the next day. There is never an evacuation drill. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [05:48<23:12, 348.11s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [09:07<16:39, 333.27s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [14:32<11:04, 332.45s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [17:53<05:19, 319.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [25:54<00:00, 335.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "STEP: 4\n",
            "\n",
            "\n",
            "Their house caught on fire. They were lucky, because they didn’t have to evacuate until the next day. There is never an evacuation drill. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n",
            "It's not necessary anymore. The firemen have left the building because the siren hasn't been turned on. The siren is too loud. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n",
            "They were lucky since they could go out into the streets safely. They were lucky, because they didn’t have to evacuate until the next day. There is never an evacuation drill. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n",
            "They could have gone to a shelter or a hotel instead of leaving right away. They were lucky their house wasn’t destroyed by the storm. The storm was approaching too quickly for them to leave later. The storm was coming closer so they left early. They were lucky because they were able to evacuate. They felt lucky they had evacuated when they did..\n",
            "They were allowed to leave because they were so close to the fire station. They were lucky because they escaped the fire at the last minute. There is never an evacuation drill. The fire department told them it was safe to leave. They were lucky that they were able to escape. They felt lucky they had evacuated when they did..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-6e5d4bd6f30d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSTEP: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mbeams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-808ffba7b8e5>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(beams, width, diversity_width, graph)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(story)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mextensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-80d4e26e34fd>\u001b[0m in \u001b[0;36mget_questions\u001b[0;34m(story)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Good Questions: Why\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_bad_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestions_bad_words_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0;31m#Get questions out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-32132728f8b7>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(ids, max_length, horizon, horizon_penalty, beams, extra_bad_words, repetition_penalty)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0mmodel_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhorizon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeams\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad_words_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbad_words_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mhorizon_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, **model_kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m             )\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m             )\n\u001b[1;32m   1738\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    972\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    847\u001b[0m                     \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m                 )\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1uv_aWgE4Tg",
        "outputId": "959345d3-2085-4a5d-f015-9f2192bedc6e"
      },
      "source": [
        "f.attr(rankdir='TB', size='100,100')\n",
        "f.engine = 'dot'\n",
        "f.ratio=\"fill\"\n",
        "f.format=\"gv\"\n",
        "f.node_attr['fixedsize'] = 'false'\n",
        "f.attr(fontsize='100')\n",
        "#print(f.node)\n",
        "#f.node_attr['width'] = '7'\n",
        "f.node_attr['height'] = '3'\n",
        "f.render(\"tg\")\n",
        "\n",
        "!unflatten -l 4 -f tg.gv | dot -Tpdf -o wide.pdf\n",
        "!ls"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distilgpt2-ranker-roc\t sample_data  tg.gv.gv\ttg.png.gv   wide.pdf\n",
            "distilgpt2-ranker-scifi  tg\t      tg.gz.gv\ttg.png.pdf  wide.png\n",
            "gdrive\t\t\t tg.gv\t      tg.png\ttg.png.png\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}